{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Runner Injury Prediction\n",
    "\n",
    "This notebook preprocesses both daily and weekly time-series data for training GRU/LSTM models to predict runner injuries. The preprocessing steps include:\n",
    "\n",
    "1. Data loading and initial exploration\n",
    "2. Data cleaning and handling missing values\n",
    "3. Feature normalization/standardization\n",
    "4. Sequence creation for time-series modeling (athlete-aware)\n",
    "5. Temporal train/validation split\n",
    "6. Saving preprocessed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily approach dataset shape: (42766, 73)\n",
      "\n",
      "Weekly approach dataset shape: (42798, 72)\n",
      "\n",
      "Daily approach columns:\n",
      "['nr. sessions', 'total km', 'km Z3-4', 'km Z5-T1-T2', 'km sprinting', 'strength training', 'hours alternative', 'perceived exertion', 'perceived trainingSuccess', 'perceived recovery', 'nr. sessions.1', 'total km.1', 'km Z3-4.1', 'km Z5-T1-T2.1', 'km sprinting.1', 'strength training.1', 'hours alternative.1', 'perceived exertion.1', 'perceived trainingSuccess.1', 'perceived recovery.1', 'nr. sessions.2', 'total km.2', 'km Z3-4.2', 'km Z5-T1-T2.2', 'km sprinting.2', 'strength training.2', 'hours alternative.2', 'perceived exertion.2', 'perceived trainingSuccess.2', 'perceived recovery.2', 'nr. sessions.3', 'total km.3', 'km Z3-4.3', 'km Z5-T1-T2.3', 'km sprinting.3', 'strength training.3', 'hours alternative.3', 'perceived exertion.3', 'perceived trainingSuccess.3', 'perceived recovery.3', 'nr. sessions.4', 'total km.4', 'km Z3-4.4', 'km Z5-T1-T2.4', 'km sprinting.4', 'strength training.4', 'hours alternative.4', 'perceived exertion.4', 'perceived trainingSuccess.4', 'perceived recovery.4', 'nr. sessions.5', 'total km.5', 'km Z3-4.5', 'km Z5-T1-T2.5', 'km sprinting.5', 'strength training.5', 'hours alternative.5', 'perceived exertion.5', 'perceived trainingSuccess.5', 'perceived recovery.5', 'nr. sessions.6', 'total km.6', 'km Z3-4.6', 'km Z5-T1-T2.6', 'km sprinting.6', 'strength training.6', 'hours alternative.6', 'perceived exertion.6', 'perceived trainingSuccess.6', 'perceived recovery.6', 'Athlete ID', 'injury', 'Date']\n",
      "\n",
      "Weekly approach columns:\n",
      "['nr. sessions', 'nr. rest days', 'total kms', 'max km one day', 'total km Z3-Z4-Z5-T1-T2', 'nr. tough sessions (effort in Z5, T1 or T2)', 'nr. days with interval session', 'total km Z3-4', 'max km Z3-4 one day', 'total km Z5-T1-T2', 'max km Z5-T1-T2 one day', 'total hours alternative training', 'nr. strength trainings', 'avg exertion', 'min exertion', 'max exertion', 'avg training success', 'min training success', 'max training success', 'avg recovery', 'min recovery', 'max recovery', 'nr. sessions.1', 'nr. rest days.1', 'total kms.1', 'max km one day.1', 'total km Z3-Z4-Z5-T1-T2.1', 'nr. tough sessions (effort in Z5, T1 or T2).1', 'nr. days with interval session.1', 'total km Z3-4.1', 'max km Z3-4 one day.1', 'total km Z5-T1-T2.1', 'max km Z5-T1-T2 one day.1', 'total hours alternative training.1', 'nr. strength trainings.1', 'avg exertion.1', 'min exertion.1', 'max exertion.1', 'avg training success.1', 'min training success.1', 'max training success.1', 'avg recovery.1', 'min recovery.1', 'max recovery.1', 'nr. sessions.2', 'nr. rest days.2', 'total kms.2', 'max km one day.2', 'total km Z3-Z4-Z5-T1-T2.2', 'nr. tough sessions (effort in Z5, T1 or T2).2', 'nr. days with interval session.2', 'total km Z3-4.2', 'max km Z3-4 one day.2', 'total km Z5-T1-T2.2', 'max km Z5-T1-T2 one day.2', 'total hours alternative training.2', 'nr. strength trainings.2', 'avg exertion.2', 'min exertion.2', 'max exertion.2', 'avg training success.2', 'min training success.2', 'max training success.2', 'avg recovery.2', 'min recovery.2', 'max recovery.2', 'Athlete ID', 'injury', 'rel total kms week 0_1', 'rel total kms week 0_2', 'rel total kms week 1_2', 'Date']\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "day_df = pd.read_csv('data/day_approach_maskedID_timeseries.csv')\n",
    "week_df = pd.read_csv('data/week_approach_maskedID_timeseries.csv')\n",
    "\n",
    "# Convert Date column to datetime\n",
    "day_df['Date'] = pd.to_datetime(day_df['Date'])\n",
    "week_df['Date'] = pd.to_datetime(week_df['Date'])\n",
    "\n",
    "print(\"Daily approach dataset shape:\", day_df.shape)\n",
    "print(\"\\nWeekly approach dataset shape:\", week_df.shape)\n",
    "\n",
    "print(\"\\nDaily approach columns:\")\n",
    "print(day_df.columns.tolist())\n",
    "print(\"\\nWeekly approach columns:\")\n",
    "print(week_df.columns.tolist())\n",
    "\n",
    "# Print date range\n",
    "print(\"\\nDaily approach date range:\")\n",
    "print(f\"Start: {day_df['Date'].min()}\")\n",
    "print(f\"End: {day_df['Date'].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences_by_athlete(df, sequence_length, features, scaler=None):\n",
    "    \"\"\"\n",
    "    Create sequences for each athlete separately to prevent data leakage between athletes.\n",
    "    \n",
    "    Args:\n",
    "        df: pandas DataFrame containing the athlete data\n",
    "        sequence_length: number of time steps to use for each sequence\n",
    "        features: list of feature columns\n",
    "        scaler: fitted StandardScaler (optional, for validation data)\n",
    "    \n",
    "    Returns:\n",
    "        X: preprocessed features array of shape (n_samples, sequence_length, n_features)\n",
    "        y: labels array of shape (n_samples,)\n",
    "        scaler: fitted StandardScaler object (if not provided)\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    # If scaler not provided, fit on this data\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(df[features])\n",
    "    \n",
    "    # Process each athlete separately\n",
    "    for athlete_id in df['Athlete ID'].unique():\n",
    "        # Get athlete data sorted by date\n",
    "        athlete_data = df[df['Athlete ID'] == athlete_id].sort_values('Date')\n",
    "        \n",
    "        # Scale features for this athlete\n",
    "        scaled_features = scaler.transform(athlete_data[features])\n",
    "        \n",
    "        # Create sequences only within same athlete's data\n",
    "        for i in range(len(scaled_features) - sequence_length):\n",
    "            seq = scaled_features[i:(i + sequence_length)]\n",
    "            label = athlete_data['injury'].iloc[i + sequence_length]\n",
    "            \n",
    "            sequences.append(seq)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return np.array(sequences), np.array(labels), scaler\n",
    "\n",
    "def preprocess_timeseries(df, sequence_length=7, is_daily=True, train_end_date=None):\n",
    "    \"\"\"\n",
    "    Preprocess the timeseries data for GRU/LSTM modeling.\n",
    "    \n",
    "    Args:\n",
    "        df: pandas DataFrame containing the timeseries data\n",
    "        sequence_length: number of time steps to use for each sequence\n",
    "        is_daily: boolean indicating if this is daily data (True) or weekly data (False)\n",
    "        train_end_date: datetime to split train/validation (if None, uses temporal split)\n",
    "    \n",
    "    Returns:\n",
    "        X_train: training features array\n",
    "        X_val: validation features array\n",
    "        y_train: training labels array\n",
    "        y_val: validation labels array\n",
    "        scaler: fitted StandardScaler object\n",
    "    \"\"\"\n",
    "    # Replace -0.01 values (missing data indicators) with NaN\n",
    "    feature_cols = df.columns.drop(['injury', 'Athlete ID', 'Date']).tolist()\n",
    "    df[feature_cols] = df[feature_cols].replace(-0.01, np.nan)\n",
    "    \n",
    "    # Handle missing values by forward fill within each athlete's data\n",
    "    df[feature_cols] = df.groupby('Athlete ID')[feature_cols].fillna(method='ffill')\n",
    "    # Any remaining NaN (at start of sequences) fill with 0\n",
    "    df[feature_cols] = df[feature_cols].fillna(0)\n",
    "    \n",
    "    # If train_end_date not provided, use 80% of the date range\n",
    "    if train_end_date is None:\n",
    "        date_range = df['Date'].max() - df['Date'].min()\n",
    "        train_end_date = df['Date'].min() + date_range * 0.8\n",
    "    \n",
    "    # Split data temporally\n",
    "    train_df = df[df['Date'] <= train_end_date].copy()\n",
    "    val_df = df[df['Date'] > train_end_date].copy()\n",
    "    \n",
    "    # Create sequences for training data\n",
    "    X_train, y_train, scaler = create_sequences_by_athlete(\n",
    "        train_df, sequence_length, feature_cols\n",
    "    )\n",
    "    \n",
    "    # Create sequences for validation data using the same scaler\n",
    "    X_val, y_val, _ = create_sequences_by_athlete(\n",
    "        val_df, sequence_length, feature_cols, scaler\n",
    "    )\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val, scaler, feature_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily approach preprocessed data shapes:\n",
      "X_daily shape: (42759, 7, 70)\n",
      "y_daily shape: (42759,)\n",
      "\n",
      "Weekly approach preprocessed data shapes:\n",
      "X_weekly shape: (42794, 4, 69)\n",
      "y_weekly shape: (42794,)\n"
     ]
    }
   ],
   "source": [
    "# Process daily approach data\n",
    "X_daily_train, X_daily_val, y_daily_train, y_daily_val, scaler_daily, daily_features = preprocess_timeseries(\n",
    "    day_df, \n",
    "    sequence_length=7,  # Use 7 days of data to predict next day\n",
    "    is_daily=True\n",
    ")\n",
    "\n",
    "# Process weekly approach data\n",
    "X_weekly_train, X_weekly_val, y_weekly_train, y_weekly_val, scaler_weekly, weekly_features = preprocess_timeseries(\n",
    "    week_df, \n",
    "    sequence_length=4,  # Use 4 weeks of data to predict next week\n",
    "    is_daily=False\n",
    ")\n",
    "\n",
    "print(\"Daily approach preprocessed data shapes:\")\n",
    "print(\"X_daily_train shape:\", X_daily_train.shape)\n",
    "print(\"X_daily_val shape:\", X_daily_val.shape)\n",
    "print(\"y_daily_train shape:\", y_daily_train.shape)\n",
    "print(\"y_daily_val shape:\", y_daily_val.shape)\n",
    "\n",
    "print(\"\\nWeekly approach preprocessed data shapes:\")\n",
    "print(\"X_weekly_train shape:\", X_weekly_train.shape)\n",
    "print(\"X_weekly_val shape:\", X_weekly_val.shape)\n",
    "print(\"y_weekly_train shape:\", y_weekly_train.shape)\n",
    "print(\"y_weekly_val shape:\", y_weekly_val.shape)\n",
    "\n",
    "# Print class distribution\n",
    "print(\"\\nDaily approach class distribution:\")\n",
    "print(\"Train - Injury rate: {:.2f}%\".format(y_daily_train.mean() * 100))\n",
    "print(\"Val - Injury rate: {:.2f}%\".format(y_daily_val.mean() * 100))\n",
    "\n",
    "print(\"\\nWeekly approach class distribution:\")\n",
    "print(\"Train - Injury rate: {:.2f}%\".format(y_weekly_train.mean() * 100))\n",
    "print(\"Val - Injury rate: {:.2f}%\".format(y_weekly_val.mean() * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily approach train/val shapes:\n",
      "X_daily_train: (34207, 7, 70)\n",
      "X_daily_val: (8552, 7, 70)\n",
      "y_daily_train: (34207,)\n",
      "y_daily_val: (8552,)\n",
      "\n",
      "Weekly approach train/val shapes:\n",
      "X_weekly_train: (34235, 4, 69)\n",
      "X_weekly_val: (8559, 4, 69)\n",
      "y_weekly_train: (34235,)\n",
      "y_weekly_val: (8559,)\n",
      "\n",
      "Daily approach class distribution:\n",
      "Train - Injury rate: 1.36%\n",
      "Val - Injury rate: 1.37%\n",
      "\n",
      "Weekly approach class distribution:\n",
      "Train - Injury rate: 1.34%\n",
      "Val - Injury rate: 1.34%\n"
     ]
    }
   ],
   "source": [
    "# Create preprocessed data directory if it doesn't exist\n",
    "os.makedirs('preprocessed_data', exist_ok=True)\n",
    "\n",
    "# Save preprocessed data\n",
    "np.save('preprocessed_data/X_daily_train.npy', X_daily_train)\n",
    "np.save('preprocessed_data/X_daily_val.npy', X_daily_val)\n",
    "np.save('preprocessed_data/y_daily_train.npy', y_daily_train)\n",
    "np.save('preprocessed_data/y_daily_val.npy', y_daily_val)\n",
    "\n",
    "np.save('preprocessed_data/X_weekly_train.npy', X_weekly_train)\n",
    "np.save('preprocessed_data/X_weekly_val.npy', X_weekly_val)\n",
    "np.save('preprocessed_data/y_weekly_train.npy', y_weekly_train)\n",
    "np.save('preprocessed_data/y_weekly_val.npy', y_weekly_val)\n",
    "\n",
    "# Save feature names and scalers\n",
    "import pickle\n",
    "\n",
    "with open('preprocessed_data/daily_features.pkl', 'wb') as f:\n",
    "    pickle.dump(daily_features, f)\n",
    "    \n",
    "with open('preprocessed_data/weekly_features.pkl', 'wb') as f:\n",
    "    pickle.dump(weekly_features, f)\n",
    "    \n",
    "with open('preprocessed_data/scaler_daily.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_daily, f)\n",
    "    \n",
    "with open('preprocessed_data/scaler_weekly.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_weekly, f)\n",
    "\n",
    "print(\"Preprocessed data saved to 'preprocessed_data' directory\")\n",
    "print(\"\\nFiles saved:\")\n",
    "print(os.listdir('preprocessed_data'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved to 'preprocessed_data' directory\n",
      "\n",
      "Files saved:\n",
      "['weekly_features.pkl', 'X_daily_val.npy', 'y_weekly_val.npy', 'daily_features.pkl', 'y_daily_train.npy', 'y_weekly_train.npy', 'X_daily_train.npy', 'scaler_weekly.pkl', 'X_weekly_train.npy', 'scaler_daily.pkl', 'y_daily_val.npy', 'X_weekly_val.npy']\n"
     ]
    }
   ],
   "source": [
    "# Create preprocessed data directory if it doesn't exist\n",
    "os.makedirs('preprocessed_data', exist_ok=True)\n",
    "\n",
    "# Save preprocessed data\n",
    "np.save('preprocessed_data/X_daily_train.npy', X_daily_train)\n",
    "np.save('preprocessed_data/X_daily_val.npy', X_daily_val)\n",
    "np.save('preprocessed_data/y_daily_train.npy', y_daily_train)\n",
    "np.save('preprocessed_data/y_daily_val.npy', y_daily_val)\n",
    "\n",
    "np.save('preprocessed_data/X_weekly_train.npy', X_weekly_train)\n",
    "np.save('preprocessed_data/X_weekly_val.npy', X_weekly_val)\n",
    "np.save('preprocessed_data/y_weekly_train.npy', y_weekly_train)\n",
    "np.save('preprocessed_data/y_weekly_val.npy', y_weekly_val)\n",
    "\n",
    "# Save feature names and scalers\n",
    "import pickle\n",
    "\n",
    "with open('preprocessed_data/daily_features.pkl', 'wb') as f:\n",
    "    pickle.dump(daily_features, f)\n",
    "    \n",
    "with open('preprocessed_data/weekly_features.pkl', 'wb') as f:\n",
    "    pickle.dump(weekly_features, f)\n",
    "    \n",
    "with open('preprocessed_data/scaler_daily.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_daily, f)\n",
    "    \n",
    "with open('preprocessed_data/scaler_weekly.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_weekly, f)\n",
    "\n",
    "print(\"Preprocessed data saved to 'preprocessed_data' directory\")\n",
    "print(\"\\nFiles saved:\")\n",
    "print(os.listdir('preprocessed_data'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
